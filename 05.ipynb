{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuters-21578 collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the collection is important for the data science.Lets have the quick look at the reuters collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 10788\n",
      "Total train documents: 7769\n",
      "Total test documents: 3019\n"
     ]
    }
   ],
   "source": [
    "# list of document ids\n",
    "documents = reuters.fileids()\n",
    "print(\"Documents: {}\".format(len(documents)))\n",
    "\n",
    "# Train documents\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"),documents))\n",
    "print(\"Total train documents: {}\".format(len(train_docs_id)))\n",
    "\n",
    "# Test documents\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"),documents))\n",
    "print(\"Total test documents: {}\".format(len(test_docs_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRENCH FREE MARKET CEREAL EXPORT BIDS DETAILED\n",
      "  French operators have requested licences\n",
      "  to export 675,500 tonnes of maize, 245,000 tonnes of barley,\n",
      "  22,000 tonnes of soft bread wheat and 20,000 tonnes of feed\n",
      "  wheat at today's European Community tender, traders said.\n",
      "      Rebates requested ranged from 127.75 to 132.50 European\n",
      "  Currency Units a tonne for maize, 136.00 to 141.00 Ecus a tonne\n",
      "  for barley and 134.25 to 141.81 Ecus for bread wheat, while\n",
      "  rebates requested for feed wheat were 137.65 Ecus, they said.\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "['barley', 'corn', 'grain', 'wheat']\n"
     ]
    }
   ],
   "source": [
    "doc = 'training/9865'\n",
    "\n",
    "print(reuters.raw(doc))\n",
    "print()\n",
    "\n",
    "print(reuters.categories(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 90\n",
      "\n",
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n",
      "\n",
      "Most common categories\n",
      "[('earn', 3964),\n",
      " ('acq', 2369),\n",
      " ('money-fx', 717),\n",
      " ('grain', 582),\n",
      " ('crude', 578)]\n",
      "\n",
      "Last common categories\n",
      "[('castor-oil', 2), ('groundnut-oil', 2), ('lin-oil', 2), ('rye', 2), ('sun-meal', 2)]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "\n",
    "# List of categories\n",
    "categories = reuters.categories()\n",
    "print(\"Number of categories: {}\".format(len(categories)))\n",
    "print()\n",
    "\n",
    "print(categories)\n",
    "print()\n",
    "\n",
    "# Documents per category\n",
    "category_distributionn = [(category,len(reuters.fileids(category))) for category in categories]\n",
    "category_distributionn = sorted(category_distributionn,key=itemgetter(1),reverse=True)\n",
    "\n",
    "print(\"Most common categories\")\n",
    "pprint(category_distributionn[:5])\n",
    "print()\n",
    "\n",
    "print(\"Last common categories\")\n",
    "print(category_distributionn[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels assigned: 3126\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"),documents))\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"),documents))\n",
    "\n",
    "train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    "\n",
    "# Tokenization\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "vectorised_train_documents = vectorizer.fit_transform(train_docs)\n",
    "vectorised_test_documents = vectorizer.transform(test_docs)\n",
    "\n",
    "# Transform multilabels labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([reuters.categories(doc_id) for doc_id in train_docs_id])\n",
    "test_labels = mlb.transform([reuters.categories(doc_id) for doc_id in test_docs_id])\n",
    "\n",
    "# Classifier\n",
    "classifier = OneVsRestClassifier(LinearSVC(random_state=42))\n",
    "classifier.fit(vectorised_train_documents,train_labels)\n",
    "\n",
    "predictions = classifier.predict(vectorised_test_documents)\n",
    "#\n",
    "print(\"Number of labels assigned: {}\".format(sum([sum(prediction) for prediction in predictions])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.9517,Recall: 0.7946,F1-measure: 0.8661\n",
      "Macro-average quality numbers\n",
      "Precision: 0.6305, Recall: 0.3715, F1-measure: 0.4451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denny\\Anaconda3\\envs\\opencv-env\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Denny\\Anaconda3\\envs\\opencv-env\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "\n",
    "precision = precision_score(test_labels,predictions,average='micro')\n",
    "recall = recall_score(test_labels,predictions,average='micro')\n",
    "f1 = f1_score(test_labels,predictions,average='micro')\n",
    "print('Micro-average quality numbers')\n",
    "print(\"Precision: {:.4f},Recall: {:.4f},F1-measure: {:.4f}\".format(precision,recall,f1))\n",
    "\n",
    "precision = precision_score(test_labels, predictions, average='macro')\n",
    "recall = recall_score(test_labels, predictions, average='macro')\n",
    "f1 = f1_score(test_labels, predictions, average='macro')\n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, \n",
    "                                                                     recall, \n",
    "                                                                     f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
